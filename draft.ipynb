{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://m.facebook.com/groups/DeepNetGroup/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  Returns the first element found that has an attribute containing given pattern \n",
    "  params:\n",
    "    - elements: an iterable containing the Webdriver elements\n",
    "    - attribute: The attribute of the html element where to check for pattern\n",
    "    - pattern: a string to verify if it is in attribute\n",
    "  returns:\n",
    "    - html element if its attribute contains the pattern\n",
    "    - None else\n",
    "\"\"\" \n",
    "\n",
    "def get_element_with_pattern_in_attribute(elements, attribute, pattern):\n",
    "    els = []\n",
    "    for element in elements:\n",
    "        try:\n",
    "            if pattern in element.get_attribute(attribute):\n",
    "                els.append(element)\n",
    "        except:\n",
    "            pass\n",
    "    return els"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "fp = webdriver.FirefoxProfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox(firefox_profile=fp)\n",
    "browser.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PAGE_FETCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Page 0\n",
      "11\n",
      "Page 1\n",
      "17\n",
      "Page 2\n",
      "23\n",
      "Page 3\n",
      "30\n",
      "Page 4\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "posts = []\n",
    "\n",
    "\n",
    "root = browser.find_element_by_id(\"m_group_stories_container\")\n",
    "els = get_element_with_pattern_in_attribute(root.find_elements_by_tag_name(\"a\"), \"text\", \"Full Story\")\n",
    "\n",
    "for el in els: \n",
    "    posts.append(el.get_attribute(\"href\"))\n",
    "    \n",
    "print(len(posts))\n",
    "\n",
    "for i in range(MAX_PAGE_FETCH):\n",
    "\n",
    "    wait = WebDriverWait(browser, 10)\n",
    "    timeline = wait.until(EC.presence_of_element_located((By.ID, 'm_group_stories_container')))\n",
    "  \n",
    "    #spans = \n",
    "    print(f\"Page {i}\") \n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    for span in browser.find_elements_by_tag_name(\"span\"):\n",
    "        if \"See More Posts\" in span.text:\n",
    "            break\n",
    "    span.click()\n",
    "\n",
    "    root = browser.find_element_by_id(\"m_group_stories_container\")\n",
    "    els = get_element_with_pattern_in_attribute(root.find_elements_by_tag_name(\"a\"), \"text\", \"Full Story\")\n",
    "   \n",
    "    for el in els: \n",
    "        posts.append(el.get_attribute(\"href\"))\n",
    "    print(len(posts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://m.facebook.com/groups/107107546348803?view=permalink&id=1061412474251634&refid=18&_ft_=qid.6781887035468863744%3Amf_story_key.1061412474251634%3Agroup_id.107107546348803%3Atop_level_post_id.1061412474251634%3Atl_objid.1061412474251634%3Acontent_owner_id_new.598099856%3Asrc.22%3Astory_location.6%3Astory_attachment_style.share%3Afilter.GroupStoriesByActivityEntQuery&__tn__=%2AW-R#footer_action_list',\n",
       " 'https://m.facebook.com/groups/107107546348803?view=permalink&id=1061539450905603&refid=18&_ft_=qid.6781887035587550102%3Amf_story_key.1061539450905603%3Agroup_id.107107546348803%3Atop_level_post_id.1061539450905603%3Atl_objid.1061539450905603%3Acontent_owner_id_new.1034174633%3Asrc.22%3Astory_location.6%3Afilter.GroupStoriesByActivityEntQuery&__tn__=%2AW-R#footer_action_list',\n",
       " 'https://m.facebook.com/groups/107107546348803?view=permalink&id=1061458150913733&refid=18&_ft_=qid.6781887035495571984%3Amf_story_key.1061458150913733%3Agroup_id.107107546348803%3Atop_level_post_id.1061458150913733%3Atl_objid.1061458150913733%3Acontent_owner_id_new.1262676285%3Aoriginal_content_id.10157649093656830%3Aoriginal_content_owner_id.43331696829%3Apage_id.43331696829%3Asrc.22%3Astory_location.6%3Aattached_story_attachment_style.share%3Afilter.GroupStoriesByActivityEntQuery%3Apage_insights.%7B%2243331696829%22%3A%7B%22page_id%22%3A43331696829%2C%22actor_id%22%3A1262676285%2C%22attached_story%22%3A%7B%22page_id%22%3A43331696829%2C%22actor_id%22%3A43331696829%2C%22dm%22%3A%7B%22isShare%22%3A1%2C%22originalPostOwnerID%22%3A0%7D%2C%22psn%22%3A%22EntStatusCreationStory%22%2C%22post_context%22%3A%7B%22object_fbtype%22%3A266%2C%22publish_time%22%3A1579011266%2C%22story_name%22%3A%22EntStatusCreationStory%22%2C%22story_fbid%22%3A%5B10157649093656830%5D%7D%2C%22role%22%3A1%2C%22sl%22%3A6%7D%2C%22dm%22%3A%7B%22isShare%22%3A1%2C%22originalPostOwnerID%22%3A0%7D%2C%22psn%22%3A%22EntGroupMallPostCreationStory%22%2C%22role%22%3A1%2C%22sl%22%3A6%2C%22targets%22%3A%5B%7B%22actor_id%22%3A1262676285%2C%22page_id%22%3A43331696829%2C%22post_id%22%3A10157649093656830%2C%22role%22%3A1%2C%22share_id%22%3A0%7D%5D%7D%7D&__tn__=%2AW-R#footer_action_list']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"bb\"><div class=\"bc bd\" data-ft=\"{&quot;top_level_post_id&quot;:&quot;1061539450905603&qu'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"bb\"><div class=\"bc bd\" data-ft=\"{&quot;top_level_post_id&quot;:&quot;1061412474251634&qu'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"bb\"><div class=\"bc bd\" data-ft=\"{&quot;top_level_post_id&quot;:&quot;1061458150913733&qu'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[2][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-110-3e804ee5f60e>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for post in tqdm_notebook(posts):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a835d112301748099a04690a76b03d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "for post in tqdm_notebook(posts):\n",
    "    browser.get(post)\n",
    "    wait = WebDriverWait(browser, 10)\n",
    "    timeline = wait.until(EC.presence_of_element_located((By.ID, 'm_story_permalink_view')))\n",
    "    roi = browser.find_element_by_id('m_story_permalink_view')\n",
    "    \n",
    "    articles.append(roi.get_attribute(\"innerHTML\"))\n",
    "    browser.execute_script(\"document.getElementById('m_story_permalink_view').remove();\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"bb\"><div class=\"bc bd\" data-ft=\"{&quot;top_level_post_id&quot;:&quot;1061539450905603&quot;,&quot;content_owner_id_new&quot;:1034174633,&quot;story_location&quot;:6,&quot;tn&quot;:&quot;-R&quot;}\" id=\"u_0_0\"><div class=\"be\"><div class=\"bf\"><table class=\"bg\" role=\"presentation\"><tbody><tr><td class=\"bh\"><div class=\"bi\"></div></td><td class=\"bj bk\"><div><h3 class=\"bl bm bn bo\"><span><strong><a>Alaa Ahmed Usama</a></strong><span class=\"bp\"> &gt; </span>\\u200e<strong><a href=\"/groups/107107546348803?refid=18&amp;_ft_=top_level_post_id.1061539450905603%3Acontent_owner_id_new.1034174633%3Astory_location.6&amp;__tn__=C-R\">Artificial Intelligence &amp; Deep Learning</a></strong></span></h3></div></td></tr></tbody></table></div><div class=\"bq\" style=\"\" data-ft=\"{&quot;tn&quot;:&quot;*s&quot;}\"><p>Hello <span class=\"br\"><span class=\"bs\" style=\"height: 16px; width: 16px; font-size: 16px; background-image: url(&quot;https://static.xx.fbcdn.net/images/emoji.php/v9/tfa/1/16/1f44b.png&quot;)\">👋</span></span><br> I want to make a collocations dictionary similar to <a href=\"https://lm.facebook.com/l.php?u=http%3A%2F%2Fozdic.com%2F&amp;h=AT2MEG4zPfeNFbQd7TmhFo0ZxoXu3ip0bKNrUF8uEcKLMi1T5Cyfc8RBg7oSkdRzVaBUkbPLPo5UX2v-ARF7GQ3k2lIPvpe3z69Y6IxBgcxPWWb16E3PkORRSSQghCqToNNw-p-MByr9GMYmlEUfkawQVmIYDlFZUIT1_3F86K80BLuZiIyAwRy_UMieFX7wm4NRyMq6gb-krifszjxkx75tIJu5_G25mWXZY8KFUT7O2ef5mEX6cFqky-YAywckxRDvjecoqSnbkJWzTY-KWWeCmh1doA\" target=\"_blank\" rel=\"noopener\">ozdic.com</a> but in Arabic.</p><p> The methodology I want to follow is to extract words that come together frequently from a number of texts, combine those that share one word together, then categorize according to parts of speech just like the aforementioned ozdic.</p><p> How viable is this? How much time would it take? And what do you suggest I do?</p></div></div><div class=\"bt\" data-ft=\"{&quot;tn&quot;:&quot;*W&quot;}\"><div class=\"bu bp\"><abbr>40 mins</abbr><span aria-hidden=\"true\"> · </span><span class=\"bv\"><div class=\"bw\"><span><span class=\"bu\">Public</span></span><div class=\"bx\"></div></div></span></div></div></div></div>'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- shared a link (with or without text before)\n",
    "- post with attachment\n",
    "- just text\n",
    "- shared a post (that last one may contain one of the previous kind of post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hypotheses : \n",
    "- data-ft='{\"tn\":\"*s\"}' contains author's text regardless of the kind of post  \n",
    "- for a post with attachment : data-ft='{\"tn\":\"E\"}' contains the link(s) to the attachment(s)\n",
    "- data-ft='{\"tn\":\"*W\"}' contains the date \n",
    "- for a shared link data-ft='{\"tn\":\"H\"}' contains the link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"bq\" data-ft='{\"tn\":\"*s\"}' style=\"\"><p>Hello <span class=\"br\"><span class=\"bs\" style='height: 16px; width: 16px; font-size: 16px; background-image: url(\"https://static.xx.fbcdn.net/images/emoji.php/v9/tfa/1/16/1f44b.png\")'>👋</span></span><br/> I want to make a collocations dictionary similar to <a href=\"https://lm.facebook.com/l.php?u=http%3A%2F%2Fozdic.com%2F&amp;h=AT2MEG4zPfeNFbQd7TmhFo0ZxoXu3ip0bKNrUF8uEcKLMi1T5Cyfc8RBg7oSkdRzVaBUkbPLPo5UX2v-ARF7GQ3k2lIPvpe3z69Y6IxBgcxPWWb16E3PkORRSSQghCqToNNw-p-MByr9GMYmlEUfkawQVmIYDlFZUIT1_3F86K80BLuZiIyAwRy_UMieFX7wm4NRyMq6gb-krifszjxkx75tIJu5_G25mWXZY8KFUT7O2ef5mEX6cFqky-YAywckxRDvjecoqSnbkJWzTY-KWWeCmh1doA\" rel=\"noopener\" target=\"_blank\">ozdic.com</a> but in Arabic.</p><p> The methodology I want to follow is to extract words that come together frequently from a number of texts, combine those that share one word together, then categorize according to parts of speech just like the aforementioned ozdic.</p><p> How viable is this? How much time would it take? And what do you suggest I do?</p></div>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(articles[0])\n",
    "\n",
    "soup.find_all('div', {'data-ft':'{\"tn\":\"*s\"}'})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Hello 👋 I want to make a collocations dictionary similar to ozdic.com but in Arabic. The methodology</title>\n",
      "<description>Hello 👋 I want to make a collocations dictionary similar to ozdic.com but in Arabic. The methodology I want to follow is to extract words that come together frequently from a number of texts, combine those that share one word together, then categorize according to parts of speech just like the aforementioned ozdic. How viable is this? How much time would it take? And what do you suggest I do?</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061539450905603</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 20:12:47 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>A perspective about AI-assisted diagnosis.</title>\n",
      "<description>A perspective about AI-assisted diagnosis.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061412474251634</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 17:52:47 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Find out how AI and deep learning are being put to use by neurosugeons.</title>\n",
      "<description>Find out how AI and deep learning are being put to use by neurosugeons.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061458150913733</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 20:12:47 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Watching this video of an autonomous car makes me really nervous. It doesn't seem to understand that</title>\n",
      "<description>Watching this video of an autonomous car makes me really nervous. It doesn't seem to understand that objects continue to exist (object permanence) and continue their movement even if you don't see them. In the representation of what the car see, pedestrians and cars continuously flicker in and out of existence and there's no anticipation of the location of pedestrians in blind spots. Around 3:34, there's someone walking in the distance, to the left. After a few moments, you can't see him/her any more until the car gets much closer. I hope that it's just the visualization that's missing details, otherwise, it might hit someone that moves suddenly. </description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061501510909397</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 20:07:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Dear people, I am doing a survey/questionnaire for my master thesis at the University of Ljubljana, </title>\n",
      "<description>Dear people, I am doing a survey/questionnaire for my master thesis at the University of Ljubljana, in which I aim to explore the travel habits of millennials. Besides statistical hypothesis testing, I plan to apply unsupervised machine learning (mainly clustering) on the gathered data, in order to structure and better understand the behavior of millennial population. I would be really grateful if you could respond to the survey (it should take about 10 to 15 minutes).  For any question or ambiguity, just drop a message or a comment. P.S. Each response is highly welcome, so please feel free to respond whether or not you are a millennial (birth year 1981 to 1996). Thanks a lot :) </description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061514717574743</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 20:07:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Here's my conversation with Daniel Kahneman, Nobel Prize winning economist and psychologist, author </title>\n",
      "<description>Here's my conversation with Daniel Kahneman, Nobel Prize winning economist and psychologist, author of Thinking Fast and Slow. We talk about two systems of thought in the human mind and their connection to deep learning, AI, and autonomous vehicles. </description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061485624244319</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 19:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Not sure if I agree with this study's conclusions.  \"When it comes to AI, recent research found that</title>\n",
      "<description>Not sure if I agree with this study's conclusions.  \"When it comes to AI, recent research found that training a large AI model—feeding large amounts of data into the computer system and asking for predictions—can emit more than 284 tonnes of carbon dioxide equivalent—nearly five times the lifetime emissions of the average American car. The results of this work show that there is a growing problem with AI's digital footprint.\" </description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060540757672139</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 18:22:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title></title>\n",
      "<description></description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061305177595697</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 17:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>The history of how overpromising and underdelivering dampened interest in AI.</title>\n",
      "<description>The history of how overpromising and underdelivering dampened interest in AI.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061301557596059</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 15:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Using a CNN and Approximate nearest neighbors, I built a reverse image search to determine a cars ma</title>\n",
      "<description>Using a CNN and Approximate nearest neighbors, I built a reverse image search to determine a cars make, model and year from over 9000 different classes. I describe how I made it here:  https://wasdkhan.github.io/2020/01/12/building-car-recognition.html And released the code here:  https://github.com/wasdkhan/car-reverse-image-search Please give me your feedback on my first AI blog post. Thanks.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061025270957021</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 14:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title></title>\n",
      "<description></description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061439357582279</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 18:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Hello! I have a question for you :)  I have a dataset of images and I am going to ask an external co</title>\n",
      "<description>Hello! I have a question for you :)  I have a dataset of images and I am going to ask an external company to develop and train a neural network for image segmentation. Is there any way to allow them to train and optimize the network WITHOUT directly giving them the images?  It was very expensive to get them and I'd rather not give them away so easily! Maybe training the network on cloud or something like that? Thanks in advance!!</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061107404282141</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 14:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Flappy bird for artificial intelligence use, like neural network & genetic algorithm, this was how i</title>\n",
      "<description>Flappy bird for artificial intelligence use, like neural network & genetic algorithm, this was how i made the game, the next video will be about ANN & GA,</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061411414251740</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 17:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Mobileye's plan.</title>\n",
      "<description>Mobileye's plan.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061413210918227</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 17:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Suppose I have an image of a cup, like in the 1st image, and I have a model that will classify which</title>\n",
      "<description>Suppose I have an image of a cup, like in the 1st image, and I have a model that will classify which pixel belongs to the top circle and which belong to the bottom circle like in the 2nd image. Notice that the model only predicts the part that it can see, it won't handle the occlusions. So the bottom circle is only half classified. From the output of the model, I want to fit an ellipse equation (or whatever that works) to both circles to obtain the 3rd image with complete circles, including the occluded parts. From the 3rd image, I will be able to find the height of an arbitrary point given on the surface of the cup by comparing it to the top point and bottom point, as seen in the 4th image. For example, the black marker represents the point I'm interested in, and if the cup is 5 cm tall, then that point is probably at 4.5 cm from the bottom. I can get this 4.5 cm number by calculating `distance(black, bottom) / distance(top, bottom) * 5cm` My problem is that I don't know how to do the fit an ellipse step, maybe I need another equation or method. If I have the measurement of the cup in real world (top circle radius, bottom circle radius, and height), how do I fit ellipse or whatever equation so that I can get the corresponding top point and bottom point when given an arbitrary point in black? The point in black also has the property telling whether it's inside the cup or outside the cup. I just want to know how high the black point is from the bottom of the cup. It's probably involving some OpenCV and ML ideas.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1059565267769688</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 02:44:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Former Obama administration advisors have warned the #Trump administration of overregulating the use</title>\n",
      "<description>Former Obama administration advisors have warned the #Trump administration of overregulating the use of #AI by private businesses.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060923770967171</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 14:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Hello. I made this Twitter list to research on medical AI. If you know someone to add, please let me</title>\n",
      "<description>Hello. I made this Twitter list to research on medical AI. If you know someone to add, please let me know. I will keep adding researchers and institutes.https://twitter.com/ishiid/lists/medical-ai</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061000484292833</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 14:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Could anyone help with https://www.reddit.com/r/learnmachinelearning/comments/eoit2n/methods_of_obje</title>\n",
      "<description>Could anyone help with https://www.reddit.com/r/learnmachinelearning/comments/eoit2n/methods_of_object_detection_segmentation_and/ ?</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061091040950444</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 14:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>It has been 6 weeks that I didn't publish my AI Weekly (a newsletter covering the latest trends in A</title>\n",
      "<description>It has been 6 weeks that I didn't publish my AI Weekly (a newsletter covering the latest trends in AI)...I had published 16 AI weekly issues until some personal emergencies demanded my full attention...If there is one thing I learned from publishing a newsletter, it would be the \"awareness of how much I don't know\"...However, I had gained confidence that if I need to learn any new area within AI, I know how to proceed and what steps to take...Today I have published my 17th AI Weekly issue..I would like you to join me in learning new topics every week by subscribing to my newsletter and stay relevant.#ai #datascience #deeplearning #artificialintelligence #machinelearninghttps://www.getrevue.co/profile/santoshgsk/issues/ai-weekly-issue-17-spotify-s-ml-platform-code-free-dl-efficient-transformers-adamod-flyte-212992</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1061231690936379</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 14:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Modern Signal Processing and Understanding disciplines (Computer Vision, NLP, Machine Learning etc) </title>\n",
      "<description>Modern Signal Processing and Understanding disciplines (Computer Vision, NLP, Machine Learning etc) are more scientific than “Physics” taught in universities. Because Physics is just about understanding the physical world and manually formulating mathematical models out of it . But modern signal processing disciplines let the computers learn, understand and model the world. A modern signal processing practitioner would let an adaptive computational model to learn out of thousands of hypotheses in the multi-dimensional hypothesis space in just few hours, in contrast to a physicist who would take decades for the same task to formulate an equivalent mathematical model.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060578687668346</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 18:21:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>🎙️ In the newest episode of Machine Learning Café, we interviewed Less Wright, who has set 13 record</title>\n",
      "<description>🎙️ In the newest episode of Machine Learning Café, we interviewed Less Wright, who has set 13 records on the fast.ai leaderboard, and we talked about an optimizer he developed, and with which he outperformed a lot of people on the leaderboard. The optimizer is called Ranger. He also talks about different strategies of using Deep Learning optimizers, it is worth to take those into consideration. We also tested Ranger and in our case, it also outperformed Adam/RAdam variants. Co-host is Levente Szabados. 🎧 You can listen to the episode here:   Apple Podcast: https://apple.co/2R55Gtc Spotify: https://spoti.fi/2QGkmzQ Webpage: http://bit.ly/MLCafe005W</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1058072357918979</link>\n",
      "<lastBuildDate>Sat, 11 Jan 2020 11:15:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Deep face lab vs Netflix for de aging the Irishman </title>\n",
      "<description>Deep face lab vs Netflix for de aging the Irishman </description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1059633247762890</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 03:07:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Hi all, I have doubt regarding CNN image classifiers which use softmax at the last layer. Let's say </title>\n",
      "<description>Hi all, I have doubt regarding CNN image classifiers which use softmax at the last layer. Let's say I have trained a CNN to classify cars, bikes and planes. So whenever I give it an image to classify, it gives out an array of probabilities for each class and they all add up to one. For example I give it an image of a ship, it will still give out some probability of being cars, bikes or planes. So my question is how can I train a CNN in such a way that it doesn't predict anything from the three classes and tells it is none of the three classes like a none of the above option? (Without having a fourth class called none) One way I can think of is a multi-output model where the number of outputs is equal to the classes and each output has a sigmoid function. So if an image's outputs are below a certain threshold it will not classify it.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1059054807820734</link>\n",
      "<lastBuildDate>Sun, 12 Jan 2020 15:06:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>The U.S. Government has taken measures to restrict exports of certain types of artificial intelligen</title>\n",
      "<description>The U.S. Government has taken measures to restrict exports of certain types of artificial intelligence software to rival powers like China.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1059955937730621</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 18:32:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>https://www.linkedin.com/pulse/google-cloud-ai-developer-relations-what-we-do-andrew-ferlitsch/?trac</title>\n",
      "<description>https://www.linkedin.com/pulse/google-cloud-ai-developer-relations-what-we-do-andrew-ferlitsch/?trackingId=rEnk%2Fq2XQSm0sTAnrNa90w%3D%3D</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060482487677966</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 15:52:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Hey guys, I'm a beginner in ML and I wanted to learn predictive models like predicting the next move</title>\n",
      "<description>Hey guys, I'm a beginner in ML and I wanted to learn predictive models like predicting the next move using current state of the game and previous moves. For this purpose I developed a minimal game (you can check the working here using Desktop view https://thumb-bat.herokuapp.com/, sorry it's only desktop oriented and lacks responsiveness yet) and now I'm trying to make use of ML inorder for the computer to make intelligent choices but I have no clue how to carry this out. Any help/resource as a starting point is appreciated.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060617017664513</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 21:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title></title>\n",
      "<description></description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060786877647527</link>\n",
      "<lastBuildDate>Tue, 14 Jan 2020 04:52:48 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>How can we implement ethics within the design of AI itself?  AIBE Summit is the largest student led </title>\n",
      "<description>How can we implement ethics within the design of AI itself?  AIBE Summit is the largest student led conference of its kind, offering insight into the latest developments in AI for students and industry leaders alike. Each week we realease an article about the latest developments in AI. Check it out and let us know your thoughts!</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060545344338347</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 18:22:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Hy, I need help. I am developing a vehicle number plate recognition system. I need state of art offl</title>\n",
      "<description>Hy, I need help. I am developing a vehicle number plate recognition system. I need state of art offline OCR to recognize characters from the license plate. I have tested tesserect (with image pre processing as well) but results were quite poor. Please help me out to suggest an OCR for number plate recognition. PS: I have to use my implementation on Raspberry pi 3, Please help me out or give me some useful resource which can help me out. [I found a darknet based OCR from github, which is generating optimal results, but Raspbery pi 3 is not supporting darknet, so I need to change my solution]. Please help me out.</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060107111048837</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 18:33:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>The data collection is growing!  As promised, the Google Sheet has been updated and we have now 70 r</title>\n",
      "<description>The data collection is growing!  As promised, the Google Sheet has been updated and we have now 70 replies.  Check the data collection here, the next update will be available once 20 other answers have been obtained→ https://drive.google.com/drive/folders/13QDBwDvlT2MXQ2oiOHdJU23eylNW5Kfx?usp=sharing In order to truly understand the Data Science process and Data Scientists approach to design, we need to get more data. Give your contribution to the research project (University of Pisa) and get a reward (5 minutes required) →  https://forms.gle/ZKMeGBZXA3hFZyf88 You will be rewarded with a selection of 10 scientific articles via email, based on your answers, that will improve your skills. Furthermore, the final data analysis will be published on GitHub.  #rstats #rstat #Python #PythonProgramming #dataviz #data #DataScientists #DataAnalytics</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1059617327764482</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 18:32:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title></title>\n",
      "<description></description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060014357724779</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 18:31:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>Check out how to improve #ReinforcementLearning  most popular algorithm Q-Learning and achieve bette</title>\n",
      "<description>Check out how to improve #ReinforcementLearning  most popular algorithm Q-Learning and achieve better #ai results:👀 https://rubikscode.net/2020/01/13/introduction-to-double-q-learning/ _________ #datascience #ai #machinelearning #deeplearning</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060113737714841</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 18:30:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>1. I have some question about Prognostics and Health Management application on PECVD gate door valve</title>\n",
      "<description>1. I have some question about Prognostics and Health Management application on PECVD gate door valve air cylinder . Who can give me some suggestion? 2. I want to predict the cylinder life time, but I have no full life cycle time from start to use until to fail at one cylinder 3. I just have one ok cylinder and another NG cylinder. OK and NG cylinder are not same parts 4. OK cylinder data (1000 counts) are more then NG cylinder data (50 counts) (Data unbalance) 5. Data description X axis is time and Y axis in pressure value 6. When gate door valve from close to open, the ok cylinder pressure value always from 0 torr to 0.5 torr 7. When gate door valve from close to open, the NG cylinder pressure value always from 0 torr to 0.3 or 0.4 torr (because NG cylinder are leak. 8. I have use OK cylinder data (1000 counts) and NG cylinder data (50 counts) to build XGB regression model, but the R2 score are -0.05, have very low accuracy 9. Should I try moving average ? or ARIMA ? logistic regression? or other good method? Can improve model accuracy ?</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1060344834358398</link>\n",
      "<lastBuildDate>Mon, 13 Jan 2020 18:26:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>First prize-winning team that includes a Kaggle Grandmaster has been disqualified for cheating\"Here </title>\n",
      "<description>First prize-winning team that includes a Kaggle Grandmaster has been disqualified for cheating\"Here is what the Bestpetting team did in the PetFinder contest:They fraudulently obtained adoption speed answers for the private test data (possibly by scraping our website)These data and answers were then encoded, obfuscated and hashed into an ID field that was disguised as part of their external \"cute-cats-and-dogs-from-pixabaycom\" datasetWhen processing the data, these hashed IDs were decoded and answers were retrieved for the predictionsOnly some of the encoded answers were used, so as to keep their final score \"realistic\"These processing codes were meticulously hidden and obfuscated under many nested layers of functions and codes, intentionally designed to be highly unreadable and seemingly mundane\"</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1057368351322713</link>\n",
      "<lastBuildDate>Fri, 10 Jan 2020 15:14:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>AI unicorn INFI raising series D at $3 billion valuation for their emotional AI core tech EmpathAI™ </title>\n",
      "<description>AI unicorn INFI raising series D at $3 billion valuation for their emotional AI core tech EmpathAI™ </description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1053757708350444</link>\n",
      "<lastBuildDate>Tue, 07 Jan 2020 02:06:00 -0000</lastBuildDate>\n",
      "\n",
      "\n",
      "<title>https://youtu.be/2UlBFiL6noU</title>\n",
      "<description>https://youtu.be/2UlBFiL6noU</description>\n",
      "<link>https://m.facebook.com/groups/107107546348803?view=permalink&id=1057426587983556</link>\n",
      "<lastBuildDate>Fri, 10 Jan 2020 16:44:00 -0000</lastBuildDate>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, a in enumerate(articles):\n",
    "    soup = BeautifulSoup(a)\n",
    "    user_text = soup.find_all('div', {'data-ft':'{\"tn\":\"*s\"}'})[0].text\n",
    "    date = soup.find_all('div', {'data-ft':'{\"tn\":\"*W\"}'})[0].text\n",
    "    \n",
    "    if len(user_text) > 100:\n",
    "        print(f\"<title>{user_text[:100]}</title>\")\n",
    "    else:\n",
    "        print(f\"<title>{user_text}</title>\")\n",
    "    \n",
    "    print(f\"<description>{user_text}</description>\")\n",
    "    post_url = posts[idx].split(\"&refid\")[0]\n",
    "    print(f\"<link>{post_url}</link>\")\n",
    "    \n",
    "    date = date.split(\"·\")[0]\n",
    "    date = dateparser.parse(date)\n",
    "    date = formatdate(float(date.strftime('%s')))\n",
    "    print(f\"<lastBuildDate>{date}</lastBuildDate>\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 14, 21, 51, 13, 117813)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateparser.parse(\"now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 14, 21, 50, 16, 120369)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateparser.parse(\"1 min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "False\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for post in posts:\n",
    "#    print(post[0].text)\n",
    "#    print(post[-1].text)\n",
    "    for el in post:\n",
    "        print(\"data-ft='{\\\"tn\\\":\\\"*W\\\"}'\" in str(el))\n",
    "        print()\n",
    "    print(\"\\n{}\\n\".format(42*\"-\"))\n",
    "    \n",
    "# Last element is always date and likes/comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv.orgarxiv.org'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[0][1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniel Kahneman: Thinking Fast and Slow, Deep Learning, and AI | Artificial Intelligence Podcastyoutube.com'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[1][1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51 mins46 · Like · 1 Comment · Full Story'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[1][2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "els = get_element_with_pattern_in_attribute(browser.find_elements_by_tag_name(\"div\"), \"role\", \"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(els)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_parser = {\n",
    "    0: lambda x: x,\n",
    "    1: lambda x: x,\n",
    "    2: lambda x: x\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_date_format(input_string):\n",
    "    month_list = [\n",
    "        \"January\",\n",
    "        \"February\",\n",
    "        \"March\",\n",
    "        \"April\",\n",
    "        \"May\",\n",
    "        \"June\",\n",
    "        \"July\",\n",
    "        \"August\",\n",
    "        \"September\",\n",
    "        \"October\",\n",
    "        \"November\",\n",
    "        \"December\",\n",
    "    ]\n",
    "    \n",
    "    for month in month_list:\n",
    "        if input_string.startswith(month):\n",
    "            if \",\" in input_string:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        elif \"Just now\" in input_string:\n",
    "            return datetime.datetime.now()\n",
    "        elif \"Yesterday\" in input_string:\n",
    "            return 3\n",
    "        elif \"hrs\" in input_string:\n",
    "            return 4\n",
    "        elif \"mins\" in input_string:\n",
    "            return 5\n",
    "        elif \"hr\" in input_string:\n",
    "            return 6\n",
    "        elif \"min\" in input_string:\n",
    "            return 7\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = \"^(\\w+) (\\d+), (\\d+) at (\\d+):(\\d+) ([APM]+)(\\d+).{3}Like.{3}(\\d+)\"\n",
    "r1 = \"^(\\w+) (\\d+) at (\\d+):(\\d+) ([APM]+)(\\d+).{3}Like.{3}(\\d+)\"\n",
    "r3 = \"Yesterday at (\\d+):(\\d+) ([AMP]+)(\\d+) .? Like .? (\\d+)\"\n",
    "r4 = \"(\\d+) hrs(\\d+) .? Like .? (\\d+)\"\n",
    "r5 = \"(\\d+) mins(\\d+) .? Like .? (\\d+)\"\n",
    "r6 = \"1 hr(\\d+) .? Like .? (\\d+)\"\n",
    "r7 = \"1 min(\\d+) .? Like .? (\\d+)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March 19 2019 1 31 PM\n",
      "2019-03-19 13:31:00\n",
      "360 Likes, 3 comments\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r0)\n",
    "m = p.match('March 19, 2019 at 1:31 PM360 · Like · 3 Comments · Full Story')\n",
    "#print(m.group(1), m.group(2), m.group(3), m.group(4), m.group(5), m.group(6))\n",
    "\n",
    "month = m.group(1)\n",
    "day = m.group(2) if len(m.group(2)) == 2 else \"0\" + m.group(2)\n",
    "year = m.group(3)\n",
    "hour = m.group(4) if len(m.group(4)) == 2 else \"0\" + m.group(4)\n",
    "minutes = m.group(5) if len(m.group(5)) == 2 else \"0\" + m.group(5)\n",
    "am_pm = m.group(6)\n",
    "\n",
    "string = f\"{month} {day} {year} {hour} {minutes} {am_pm}\"\n",
    "date = datetime.datetime.strptime(string, \"%B %d %Y %I %M %p\")\n",
    "print(date)\n",
    "print(f\"{m.group(7)} Likes, {m.group(8)} comments\")\n",
    "#datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 9 11 20 PM 36\n",
      "2020-01-09 23:20:00\n",
      "36 Likes, 3 comments\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r1)\n",
    "m = p.match('January 9 at 11:20 PM36 · Like · 3 Comments · Full Story')\n",
    "print(m.group(1), m.group(2), m.group(3), m.group(4), m.group(5), m.group(6))\n",
    "\n",
    "month = m.group(1)\n",
    "day = m.group(2) if len(m.group(2)) == 2 else \"0\" + m.group(2)\n",
    "year = datetime.datetime.now().year\n",
    "hour = m.group(3) if len(m.group(3)) == 2 else \"0\" + m.group(3)\n",
    "minutes = m.group(4) if len(m.group(4)) == 2 else \"0\" + m.group(4)\n",
    "am_pm = m.group(5)\n",
    "\n",
    "string = f\"{month} {day} {year} {hour} {minutes} {am_pm}\"\n",
    "date = datetime.datetime.strptime(string, \"%B %d %Y %I %M %p\")\n",
    "print(date)\n",
    "print(f\"{m.group(6)} Likes, {m.group(7)} comments\")\n",
    "#datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 15 PM 7 3\n",
      "2020-01-11 12:15:00\n",
      "7 Likes, 3 comments\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r3)\n",
    "m = p.match('Yesterday at 12:15 PM7 · Like · 3 Comments · Full Story')\n",
    "print(m.group(1), m.group(2), m.group(3), m.group(4), m.group(5))\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "\n",
    "month = str(today.month)\n",
    "month = month if len(month) == 2 else \"0\" + month\n",
    "day = str(today.day)\n",
    "day = day if len(day) == 2 else \"0\" + day\n",
    "\n",
    "year = datetime.datetime.now().year\n",
    "hour = m.group(1) if len(m.group(1)) == 2 else \"0\" + m.group(1)\n",
    "minutes = m.group(2) if len(m.group(2)) == 2 else \"0\" + m.group(2)\n",
    "am_pm = m.group(3)\n",
    "\n",
    "string = f\"{month} {day} {year} {hour} {minutes} {am_pm}\"\n",
    "date = datetime.datetime.strptime(string, \"%m %d %Y %I %M %p\")\n",
    "date += datetime.timedelta(days=-1)\n",
    "print(date)\n",
    "print(f\"{m.group(4)} Likes, {m.group(5)} comments\")\n",
    "#datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 60 3\n",
      "2020-01-12 01:34:47.119272\n",
      "60 Likes, 3 comments\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r4)\n",
    "m = p.match('22 hrs60 · Like · 3 Comments · Full Story')\n",
    "print(m.group(1), m.group(2), m.group(3))\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "\n",
    "date = today + datetime.timedelta(hours=-int(m.group(1)))\n",
    "print(date)\n",
    "print(f\"{m.group(2)} Likes, {m.group(3)} comments\")\n",
    "#datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 60 3\n",
      "2020-01-12 23:15:25.636513\n",
      "60 Likes, 3 comments\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r5)\n",
    "m = p.match('22 mins60 · Like · 3 Comments · Full Story')\n",
    "print(m.group(1), m.group(2), m.group(3))\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "\n",
    "date = today + datetime.timedelta(minutes=-int(m.group(1)))\n",
    "print(date)\n",
    "print(f\"{m.group(2)} Likes, {m.group(3)} comments\")\n",
    "#datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1\n",
      "2020-01-12 22:49:35.912713\n",
      "6 Likes, 1 comments\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r6)\n",
    "m = p.match('1 hr6 · Like · 1 Comment · Full Story')\n",
    "print(m.group(1), m.group(2))\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "\n",
    "date = today + datetime.timedelta(hours=-1)\n",
    "print(date)\n",
    "print(f\"{m.group(1)} Likes, {m.group(2)} comments\")\n",
    "#datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1\n",
      "2020-01-12 23:41:18.408037\n",
      "6 Likes, 1 comments\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r7)\n",
    "m = p.match('1 min6 · Like · 1 Comment · Full Story')\n",
    "print(m.group(1), m.group(2))\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "\n",
    "date = today + datetime.timedelta(minutes=-1)\n",
    "print(date)\n",
    "print(f\"{m.group(1)} Likes, {m.group(2)} comments\")\n",
    "#datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.utils import formatdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sun, 12 Jan 2020 21:49:35 -0000'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatdate(float(date.strftime('%s')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 12, 29, 23, 13, 32, 923606)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today + datetime.timedelta(days=-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAQ is here! Ask basic questions at https://goo.gl/M8Kx4J (The second Announcement post.)] [Use the Group Discussion thread. You can find it in Announcements.]We are the largest and the most active FB group for Artificial Intelligence/Deep Learning, or AIDL. Please scroll down & search for the FAQ... More\n",
      "\n",
      "Arthur Chan > ‎Artificial Intelligence & Deep Learning#faqthread. Here is the FAQ thread.  All discussion on basic questions such as \"How should I start deep learning?\" will direct here.  Here are some classics: \"How do I learn deep learning/AI?\" - See Q2-Q4.  \"What programming language should I use?\" - See Q10 \"I want to build a chat bot? How do I start\" - See Q19 \"Can you suggest some ideas for my projects/thesis/courses?\" - See Q20 Note that the Pinned Post FAQ is an invaluable resource.  So consider to read it before you ask any question on the... More\n",
      "\n",
      "#faqthread. Here is the FAQ thread.  All discussion on basic questions such as \"How should I start deep learning?\" will direct here.  Here are some classics: \"How do I learn deep learning/AI?\" - See Q2-Q4.  \"What programming language should I use?\" - See Q10 \"I want to build a chat bot? How do I start\" - See Q19 \"Can you suggest some ideas for my projects/thesis/courses?\" - See Q20 Note that the Pinned Post FAQ is an invaluable resource.  So consider to read it before you ask any question on the... More\n",
      "\n",
      "March 19, 2019 at 1:31 PM360 · Like · 3 Comments · Full Story\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "#faqthread. Here is the FAQ thread.  All discussion on basic questions such as \"How should I start deep learning?\" will direct here.  Here are some classics: \"How do I learn deep learning/AI?\" - See Q2-Q4.  \"What programming language should I use?\" - See Q10 \"I want to build a chat bot? How do I start\" - See Q19 \"Can you suggest some ideas for my projects/thesis/courses?\" - See Q20 Note that the Pinned Post FAQ is an invaluable resource.  So consider to read it before you ask any question on the... More\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "What is your opinion on this? \n",
      "\n",
      "Rethinking Business Strategy in the Age of AIhbswk.hbs.edu\n",
      "\n",
      "January 9 at 11:20 PM36 · Like · 3 Comments · Full Story\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Several of us on the AI/DL list, including Adam Milton-Barker and Amitā Kapoor, are starting a non-profit organization to develop AI technologies for leukemia research. This is a nice example of people meeting on this list to start new projects. Please spread the word, and help us achieve our... More\n",
      "\n",
      "Help Fund A Leukemia AI Research Foundationindiegogo.com\n",
      "\n",
      "16 hrs41 · Like · 2 Comments · Full Story\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Computers are getting more creative. But can they write rap lyrics? This is fun and cool work with friends and colleagues at ETH Zurich, Google, MIT, and Musixmatch, that explains AI-generated rap lyrics, plus a demo song by PomDP the PhD rapper with lyrics written by artificial intelligence. \n",
      "\n",
      "Generating rap lyrics with AIblog.musixmatch.com\n",
      "\n",
      "22 hrs60 · Like · 3 Comments · Full Story\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "CALL FOR PAPERSThe 2nd International Workshop on  Machine Learning for NextGeneration Systems and Networks (MLNGSN'2020)to be held in conjunction with IEEE ISNCC 202016-18 June 2020, Montreal, Canada.http://www.isncc-conf.org/workshops/mlngsnDear Colleagues,We are pleased to invite you to submit... More\n",
      "\n",
      "MLNGSN - ISNCC2020isncc-conf.org\n",
      "\n",
      "18 hrs6 · Like · 1 Comment · Full Story\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "https://thegradient.pub/an-epidemic-of-ai-misinformation/\n",
      "\n",
      "An Epidemic of AI Misinformationthegradient.pub\n",
      "\n",
      "January 10 at 2:50 PM49 · Like · 4 Comments · Full Story\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "LEEDS Uni: Lecturer or Associate Professor, Artificial Intelligence and Data Science, deadline 5 Feb 2020 https://www.jobs.ac.uk/job/BXU092/lecturer-or-associate-professor-in-computer-science£41-59K p.a. full-time permanent post, deadline for applications: 5.2.2020we are seeking outstanding... More\n",
      "\n",
      "Lecturer or Associate Professor in Computer Science at University of Leedsjobs.ac.uk\n",
      "\n",
      "18 hrs20 · Like · 1 Comment · Full Story\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for el in els: \n",
    "    soup = BeautifulSoup(el.get_attribute(\"innerHTML\"), 'html.parser')\n",
    "    aa = soup.find_all(lambda x: x.has_attr('data-ft'))\n",
    "    for a in aa:\n",
    "        print(a.text)\n",
    "        print()\n",
    "    print(\"\\n{}\\n\".format(42*\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for tag in soup:\n",
    "    print(tag.has_attr('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = soup.find_all(lambda x: x.has_attr('data-ft'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEEDS Uni: Lecturer or Associate Professor, Artificial Intelligence and Data Science, deadline 5 Feb 2020 https://www.jobs.ac.uk/job/BXU092/lecturer-or-associate-professor-in-computer-science£41-59K p.a. full-time permanent post, deadline for applications: 5.2.2020we are seeking outstanding... More\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Lecturer or Associate Professor in Computer Science at University of Leedsjobs.ac.uk\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "18 hrs20 · Like · 1 Comment · Full Story\n",
      "\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in aa:\n",
    "    print(a.text)\n",
    "    print(\"\\n{}\\n\".format(42*\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = \"^(\\w+) (\\d+), (\\d+) at (\\d+):(\\d+) ([APM]+)(\\d+).{3}Like.{3}(\\d+)\"\n",
    "r1 = \"^(\\w+) (\\d+) at (\\d+):(\\d+) ([APM]+)(\\d+).{3}Like.{3}(\\d+)\"\n",
    "r2 = \"Just now(\\d+) .? Like .? (\\d+)\"\n",
    "r3 = \"Yesterday at (\\d+):(\\d+) ([AMP]+)(\\d+) .? Like .? (\\d+)\"\n",
    "r4 = \"(\\d+) hrs(\\d+) .? Like .? (\\d+)\"\n",
    "r5 = \"(\\d+) mins(\\d+) .? Like .? (\\d+)\"\n",
    "r6 = \"1 hr(\\d+) .? Like .? (\\d+)\"\n",
    "r7 = \"1 min(\\d+) .? Like .? (\\d+)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = [r0, r1, r2, r3, r4, r5, r6, r7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    \"Yesterday at 5:01 PM20 · Like · 1 Comment · Full Story\",\n",
    "    \"Just now20 · Like · 1 Comment · Full Story\",\n",
    "    \"1 min20 · Like · 1 Comment · Full Story\",\n",
    "    \"1 hr20 · Like · 1 Comment · Full Story\",\n",
    "    \"2 hrs20 · Like · 1 Comment · Full Story\",\n",
    "    \"2 mins20 · Like · 1 Comment · Full Story\",\n",
    "    \"December 24, 2019 at 5:46 AM225 · Like · 8 Comments · Full Story\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 33), match='Yesterday at 5:01 PM20 · Like · 1'>\n",
      "<re.Match object; span=(0, 21), match='Just now20 · Like · 1'>\n",
      "<re.Match object; span=(0, 18), match='1 min20 · Like · 1'>\n",
      "<re.Match object; span=(0, 17), match='1 hr20 · Like · 1'>\n",
      "<re.Match object; span=(0, 18), match='2 hrs20 · Like · 1'>\n",
      "<re.Match object; span=(0, 19), match='2 mins20 · Like · 1'>\n",
      "<re.Match object; span=(0, 42), match='December 24, 2019 at 5:46 AM225 · Like · 8'>\n"
     ]
    }
   ],
   "source": [
    "for test in test_cases:\n",
    "    match_found = False\n",
    "    for rg in regex:\n",
    "        p = re.compile(rg)\n",
    "        m = p.match(test)\n",
    "        if m is not None:\n",
    "            print(m)\n",
    "            match_found = True\n",
    "    if not match_found:\n",
    "        raise Exception(\"No match found for '{}'\".format(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
